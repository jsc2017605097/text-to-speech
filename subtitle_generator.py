import os
import tempfile
import time
from typing import Callable, Optional
import whisper
import torch


def generate_subtitle(audio_file: str, model_name: str = "base", log_func: Optional[Callable[[str], None]] = None) -> str:
    """
    Táº¡o file phá»¥ Ä‘á» SRT tá»« file audio sá»­ dá»¥ng Whisper local
    
    Args:
        audio_file: ÄÆ°á»ng dáº«n Ä‘áº¿n file audio
        model_name: TÃªn model Whisper (tiny, base, small, medium, large)
        log_func: HÃ m callback Ä‘á»ƒ log thÃ´ng tin
        
    Returns:
        ÄÆ°á»ng dáº«n Ä‘áº¿n file SRT Ä‘Ã£ táº¡o
    """
    
    def log(msg: str):
        if log_func:
            log_func(msg)
        else:
            print(msg)
    
    if not os.path.exists(audio_file):
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file audio: {audio_file}")
    
    log(f"ğŸ¤– Äang táº£i Whisper model '{model_name}'...")
    
    # Táº¡o tÃªn file output
    base_name = os.path.splitext(audio_file)[0]
    srt_file = f"{base_name}.srt"
    
    try:
        # Load model
        model = load_whisper_model(model_name, log_func)
        
        # Transcribe audio
        log("ğŸ¤ Äang chuyá»ƒn Ä‘á»•i audio thÃ nh text...")
        result = transcribe_audio_local(model, audio_file, log_func)
        
        # Táº¡o file SRT tá»« result
        create_srt_file(result, srt_file, log_func)
        
        log(f"âœ… ÄÃ£ táº¡o phá»¥ Ä‘á»: {srt_file}")
        return srt_file
        
    except Exception as e:
        log(f"âŒ Lá»—i khi táº¡o phá»¥ Ä‘á»: {str(e)}")
        raise


def load_whisper_model(model_name: str, log_func: Optional[Callable[[str], None]] = None):
    """
    Load Whisper model vá»›i thÃ´ng bÃ¡o tiáº¿n trÃ¬nh
    """
    def log(msg: str):
        if log_func:
            log_func(msg)
        else:
            print(msg)
    
    # Kiá»ƒm tra GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    log(f"ğŸ”§ Sá»­ dá»¥ng device: {device}")
    
    # ThÃ´ng tin vá» cÃ¡c model
    model_info = {
        "tiny": "~39MB - Nhanh nháº¥t, cháº¥t lÆ°á»£ng tháº¥p",
        "base": "~74MB - CÃ¢n báº±ng tá»‘c Ä‘á»™ vÃ  cháº¥t lÆ°á»£ng", 
        "small": "~244MB - Cháº¥t lÆ°á»£ng tá»‘t",
        "medium": "~769MB - Cháº¥t lÆ°á»£ng cao",
        "large": "~1550MB - Cháº¥t lÆ°á»£ng tá»‘t nháº¥t"
    }
    
    log(f"ğŸ“¥ Äang táº£i model '{model_name}' ({model_info.get(model_name, 'Unknown')})")
    
    try:
        model = whisper.load_model(model_name, device=device)
        log(f"âœ… ÄÃ£ táº£i model '{model_name}' thÃ nh cÃ´ng!")
        return model
    except Exception as e:
        log(f"âŒ Lá»—i khi táº£i model: {str(e)}")
        raise


def transcribe_audio_local(model, audio_file: str, log_func: Optional[Callable[[str], None]] = None) -> dict:
    """
    Transcribe audio file sá»­ dá»¥ng Whisper local model
    """
    def log(msg: str):
        if log_func:
            log_func(msg)
        else:
            print(msg)
    
    log("ğŸ”„ Äang xá»­ lÃ½ audio file...")
    
    # Transcribe vá»›i word-level timestamps
    result = model.transcribe(
        audio_file,
        language="vi",  # Tiáº¿ng Viá»‡t
        word_timestamps=True,
        verbose=False
    )
    
    log("âœ… Transcription hoÃ n táº¥t!")
    return result


def create_srt_file(result: dict, output_file: str, log_func: Optional[Callable[[str], None]] = None):
    """
    Táº¡o file SRT tá»« káº¿t quáº£ transcription cá»§a Whisper
    """
    
    def log(msg: str):
        if log_func:
            log_func(msg)
        else:
            print(msg)
    
    def format_timestamp(seconds: float) -> str:
        """Chuyá»ƒn Ä‘á»•i seconds thÃ nh Ä‘á»‹nh dáº¡ng SRT (HH:MM:SS,mmm)"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        milliseconds = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"
    
    log("ğŸ“ Äang táº¡o file SRT...")
    
    with open(output_file, "w", encoding="utf-8") as f:
        if "segments" in result:
            # Xá»­ lÃ½ tá»«ng segment
            for i, segment in enumerate(result["segments"], 1):
                start_time = format_timestamp(segment["start"])
                end_time = format_timestamp(segment["end"])
                text = segment["text"].strip()
                
                # Chia text dÃ i thÃ nh nhiá»u dÃ²ng
                lines = split_text_into_lines(text, max_chars=50)
                
                f.write(f"{i}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{lines}\n\n")
        else:
            # Fallback: táº¡o má»™t segment duy nháº¥t cho toÃ n bá»™ text
            text = result.get("text", "").strip()
            if text:
                # Æ¯á»›c tÃ­nh thá»i gian dá»±a trÃªn Ä‘á»™ dÃ i text
                words = text.split()
                duration = max(len(words) / 2.5, 5)  # Khoáº£ng 2.5 tá»«/giÃ¢y
                
                lines = split_text_into_lines(text, max_chars=50)
                
                f.write("1\n")
                f.write(f"00:00:00,000 --> {format_timestamp(duration)}\n")
                f.write(f"{lines}\n\n")
    
    log(f"âœ… ÄÃ£ lÆ°u file SRT: {output_file}")


def split_text_into_lines(text: str, max_chars: int = 50) -> str:
    """
    Chia text thÃ nh nhiá»u dÃ²ng Ä‘á»ƒ dá»… Ä‘á»c trong subtitle
    """
    words = text.split()
    lines = []
    current_line = ""
    
    for word in words:
        if len(current_line + " " + word) <= max_chars:
            current_line += (" " + word) if current_line else word
        else:
            if current_line:
                lines.append(current_line)
            current_line = word
    
    if current_line:
        lines.append(current_line)
    
    return "\n".join(lines)


def create_optimized_segments(result: dict, max_segment_length: int = 10) -> dict:
    """
    Tá»‘i Æ°u hÃ³a segments Ä‘á»ƒ cÃ³ Ä‘á»™ dÃ i phÃ¹ há»£p cho subtitle
    """
    if "segments" not in result:
        return result
    
    optimized_segments = []
    
    for segment in result["segments"]:
        duration = segment["end"] - segment["start"]
        text = segment["text"].strip()
        
        if duration <= max_segment_length:
            optimized_segments.append(segment)
        else:
            # Chia segment dÃ i thÃ nh cÃ¡c pháº§n nhá» hÆ¡n
            words = text.split()
            if len(words) <= 1:
                optimized_segments.append(segment)
                continue
                
            # Chia Ä‘á»u thá»i gian cho cÃ¡c tá»«
            time_per_word = duration / len(words)
            
            # NhÃ³m tá»« thÃ nh cÃ¡c segment ngáº¯n hÆ¡n
            words_per_segment = max(1, int(max_segment_length / time_per_word))
            
            for i in range(0, len(words), words_per_segment):
                word_group = words[i:i + words_per_segment]
                segment_text = " ".join(word_group)
                
                start_time = segment["start"] + i * time_per_word
                end_time = min(segment["start"] + (i + len(word_group)) * time_per_word, segment["end"])
                
                optimized_segments.append({
                    "start": start_time,
                    "end": end_time,
                    "text": segment_text
                })
    
    result["segments"] = optimized_segments
    return result


def check_whisper_installation():
    """
    Kiá»ƒm tra xem Whisper Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t chÆ°a
    """
    try:
        import whisper
        return True
    except ImportError:
        return False


def install_whisper_guide():
    """
    HÆ°á»›ng dáº«n cÃ i Ä‘áº·t Whisper
    """
    guide = """
    âŒ ChÆ°a cÃ i Ä‘áº·t OpenAI Whisper!
    
    Äá»ƒ sá»­ dá»¥ng tÃ­nh nÄƒng táº¡o phá»¥ Ä‘á», vui lÃ²ng cÃ i Ä‘áº·t Whisper:
    
    1. CÃ i Ä‘áº·t qua pip:
       pip install openai-whisper
    
    2. Hoáº·c cÃ i Ä‘áº·t tá»« GitHub (phiÃªn báº£n má»›i nháº¥t):
       pip install git+https://github.com/openai/whisper.git
    
    3. CÃ i Ä‘áº·t thÃªm ffmpeg (náº¿u chÆ°a cÃ³):
       - Windows: Táº£i tá»« https://ffmpeg.org/download.html
       - macOS: brew install ffmpeg
       - Linux: sudo apt install ffmpeg
    
    4. Äá»ƒ sá»­ dá»¥ng GPU (tÃ¹y chá»n, tÄƒng tá»‘c Ä‘á»™):
       pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
    """
    return guide


# HÃ m test Ä‘á»ƒ kiá»ƒm tra
def test_subtitle_generation():
    """
    HÃ m test - chá»‰ sá»­ dá»¥ng khi cÃ³ file audio há»£p lá»‡
    """
    if not check_whisper_installation():
        print(install_whisper_guide())
        return
    
    # Thay Ä‘á»•i cÃ¡c giÃ¡ trá»‹ nÃ y Ä‘á»ƒ test
    test_audio = "test_audio.mp3"
    test_model = "base"
    
    def test_log(msg):
        print(f"[TEST] {msg}")
    
    try:
        subtitle_file = generate_subtitle(test_audio, test_model, test_log)
        print(f"Test thÃ nh cÃ´ng! File SRT: {subtitle_file}")
    except Exception as e:
        print(f"Test tháº¥t báº¡i: {e}")


if __name__ == "__main__":
    print("Whisper Local Subtitle Generator")
    print("=" * 40)
    
    if check_whisper_installation():
        print("âœ… Whisper Ä‘Ã£ sáºµn sÃ ng!")
        print("Sá»­ dá»¥ng hÃ m generate_subtitle() Ä‘á»ƒ táº¡o phá»¥ Ä‘á» tá»« file audio")
    else:
        print(install_whisper_guide())
    
    # Uncomment dÃ²ng dÆ°á»›i Ä‘á»ƒ test
    # test_subtitle_generation()